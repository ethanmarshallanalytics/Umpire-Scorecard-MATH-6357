## Chapter 5

liquid = read.csv(choose.files(), header = T) # File name is 'Liquidity'
attach(liquid)
head(liquid)
cor(cbind(VOLUME, AVGT, NTRAN, PRICE, SHARE, VALUE, DEBEQ))

# Lets Try Stepwise Regression:
m0 = lm(VOLUME ~ 1)
# Null model with no X variables in it
summary(m0)

m1 = lm(VOLUME ~ AVGT + NTRAN + PRICE + SHARE + VALUE + DEBEQ)
#Full model with all of the X variable choices in it
summary(m1)

step(m0, scope=list(lower=m0, upper=m1, direction = "both"))
# The step command (while actually covers Stepwise, Forward, and Backwards) has (starting point model, with
#   the lowe rand upper choices as the min and max varaibles to choose from and the 'both' is the moethod
#   command for choosing stepwise since it does both functions of forward and backward by putting
#   variables in and throwing variables out.)

# INTERPRET OUTPUT:
# Through the output, the <none> entries are decisions that would make the model worse if they are below 
#   the <none> and would be better (want to add to model) if they are above the <none>. + are for 
#   adding variables to the model and - are for throwing variables away from the model.
#   It gives the AIC values as you go along. The stopping point is the final model that stepwise
#   has chosen, along with the values for b0 and the slopes.

mStep = lm(VOLUME ~ NTRAN + AVGT + SHARE)
summary(mStep)

# R2 went down a little from 0.849 to 0.8469, but not much. R2a went up from 0.8412 to 0.8431 since we 
#   are taken less of a penalty hit for fewer X's, so that's a little better. s went down a little from
#   4.237 to 4.212, which is a little better. All with only 3 X's instead of 6.

############################ BACKWARDS REGRESSION ####################################
# Let's try Backwards Regression:
summary(m1)

# PRICE is the worst variable right now and the least significiant at predicting VOLUME since it has 
#   the highest p-value at 0.54562 > any alpha (even the typical max of alpha = 0.10)
#   We throw it out and run the model again.

m1b = lm(VOLUME ~ AVGT + NTRAN + SHARE + VALUE + DEBEQ)
summary(m1b)

# VALUE is the worst variable right now and the least significant at predicting VOLUME since it has 
#   the highest p-value at 0.658940 > any alpha (even the typical max of alpha = 0.10) Throw it out!

m1c = lm(VOLUME ~ AVGT + NTRAN + SHARE + DEBEQ)
summary(m1c)

# Throw out DEBEQ (p-value = 0.307424)
m1d = lm(VOLUME ~ AVGT + NTRAN + SHARE)
summary(m1d)

# If alpha = 0.10 or 0.05, we would stop here since all of the X's are significant with
#   t p-values < alpha and this would be our model. 

# If alpha = 0.01, we would throw out SHARE at p-value = 0.0484. Let's try that and see what happens
m1e = lm(VOLUME ~ AVGT + NTRAN)
summary(m1e)

# Throw out AVGT 0.018205
m1f = lm(VOLUME ~ NTRAN)
summary(m1f)
# If alpha = 0.01, we would stop here and this would be the regression model.

# Originally: s=4.237, R2 = 0.849, R2a = 0.841, F p-value = 0
# If alpha=0.1 or 0.05: s=4.212, R2=.8469, R2a=.8431, F p-value = 0
# If alpha = 0.01: s = 4.346, R2=.8343, R2a=.8329, F p-value = 0

# There is no right answer here as to which model is the best since different models have the
#   best ( and lowest s, highest R2, highest R2a, and lowest F p-value). However, keep in mind that
#   it is also desirable to have only significant X's in the model (which rules out m1) and to have as
#   simple model as possible (which makes m1f desirable). 

step(m1, scope=list(lower=m0, upper=m1, direction = "backwards")) # BACKWARDS REGRESSION

# In the future if I ask for the model or equation that is chosen, it needs variable names and numbers
#   in it so it can be solved. Let's say we chose model m1f, the model/equation is:

VOLUME = 1.6512835 + 0.0018289(NTRAN)

###################### NOVEMBER 2 ###########################
liquid = read.csv(choose.files(), header = T) # File name is 'Liquidity'
attach(liquid)
m1 = lm(VOLUME ~ AVGT + NTRAN + PRICE + SHARE + VALUE + DEBEQ)

# Scatterplot Matrix
Vars = data.frame(AVGT, NTRAN, PRICE, SHARE, VALUE, DEBEQ, VOLUME)
pairs(Vars, upper.panel = NULL)

# Note that we seem to have 1 point in the top corner of the plots. This will probably be an outlier 
#   and this is affecting all or most of the X variables. So, many of them will imporove after that 
#   point is removed. 

# Let me also show you how to make a grpah with the observation numbers on it.

plot(SHARE, VOLUME, type="n")
# This will technically build the graph, but doesn't put the points on there.
text(SHARE, VOLUME, labels=rownames(liquid))

# Let's find the outliers, points that are unusual in the Y direction:
# Standardized Residuals:
rsta = rstandard(m1)
rsta[order(rsta)]

# Observation 79, 122, 9+,103,42,59,85,37 are all outliers since their standardized residual values > |2|

# Now let's find the studentized residuals:
rstu = rstudent(m1)
rstu[order(rstu)]

# The studentized resdiuals identifies all of the same points as outliers. This is often, but not always,
#   the case. 

liquid[which(rstandard(m1)>2.0),]

liquid[which(rstandard(m1)<-2.0),]
# This didn't work because <- is the arrow assignment indicator
liquid[which(abs(rstandard(m1))> 2.0),]

# Now let's remove the outliers and rerun our model and see what it looks like.
liquid1 = subset(liquid, rsta <2&rsta>-2)
# This will keep all of the points that are not outliers, that are inside the +/- 2 cutoff

dim(liquid)
dim(liquid1)

attach(liquid1)
m2 = lm(VOLUME ~ AVGT + NTRAN + PRICE + SHARE + VALUE + DEBEQ)
summary(m1)
summary(m2)

# All of the goodness of fit statistics items got better.
# s went down from 4.237 to 3.151, R2 went up from .849 to .890, R2a went up from .841 to .884,
#   and the f p-value is still 0. Also now, the p-value for SHARE got better from .2206 to .0516.
#   This X variable would now be significant at alpha = .10

########################## NOVEMBER 4 ###########################
# Now let's find the high leverage points, those unusual in the X direction. 

lev = hatvalues(m1)
lev[order(lev)]

# Cutoff for a high leverage point = 3(k+1)/n (n = # of rows and k = # of variables)
dim(liquid)
summary(m1)

cutoff = 3*(6+1)/123
print(cutoff)

# High Leverage Points that eceed this cutoff are 112, 94, 77, 53, 122, 38, 60

liquid[which(hatvalues(m1)>.1707),] #Gives us the cutoff points and gives us their data

cooksd = cooks.distance(m1)
cooksd[order(cooksd)]

#df1 = k+1 = 6+1 = 7
#df2 = n-(k+1) = 123-(6+1) = 116
qf(.95,7,116) # using alpha  = 0.05

# Any Cook's D value that exceeds 2.089 would be declared overly influential, our largest value was observation 60
#   with 1.189. So none of our points exceed the cutoff. 

Vars = data.frame(AVGT, NTRAN, PRICE, SHARE, VALUE, DEBEQ, VOLUME)
pairs(Vars, upper.panel = NULL)

# Now let's remove the outliers and high leverage points we found from the dataset, then redo our graphs and model. 

liquid1 = subset(liquid, lev<.1707 & rsta<2 & rsta>-2)
dim(liquid)
dim(liquid1)
attach(liquid1)
# Rerun Scatterplot Matrix
Vars = data.frame(AVGT, NTRAN, PRICE, SHARE, VALUE, DEBEQ, VOLUME)
pairs(Vars, upper.panel = NULL)

m2 = lm(VOLUME ~ AVGT + NTRAN + PRICE + SHARE + VALUE + DEBEQ)
summary(m2)

# We can see in the new scatterplots after the outliers and high leverage points have been removed that the patterns
#   may look similar in a lot of the graphs, but many of the ranges are different since points around the edges were
#   trimmed off.

# In the goodness of fit items, s has gone down form 4.237 to 3.115, which is better. R2 went down from .849 to .839,
#   which is a little worse. R2a, went down from .8412 to .8296, which is also a little worse. (But, both are still
#   very close.) F p-value was 0 before and is 0 now. 

# AVGT and NTRAN are both still significant at alpha =  0.05 since their p-values are 0.00612 and 2e-13 respectively.
#   PRICE< SHARE, and VALUE are all still NOT SIGNIFICANT since their p-values are greater than alpha. 
#   DEBEQ was NOT significant before with p-value = 0.306, but it is significiant not at p-value = .013

# If you remove only the outlier points, the model will almost always get better. If you also remove the high leverage
#   points, that may make the model better or worse. 

# Outlier Example:
outlier = read.csv(choose.files(), header=T) # File Name OutlierExample
attach(outlier)
# Keep in mind that outlier is unusual in the vertical (Y) direction. 
# High Leverage point is unusual in the horizontal (X) direction.

modelA = lm(Y~X, subset = -c(21,22)) # This uses the 19 'good' points and bad point 20
rstandard(modelA)[20] # Finds the standardized residual for observation 20 only
# This would be an outlier since it exceeds the cutoff of +/- 2
hatvalues(modelA)[20]
# cutoff = 3(k+1)/n
3*(1+1)/20

# This 0.067 does not exceed 0.3, so observation 20 is not a high leverage point
cooks.distance(modelA)[20]
# df1=k=1= 1+1, df2=n-(k+1)=20-(1+1)=18
qf(.95,2,18)

# Going through the process but finding the information for 21
modelB = lm(Y~X, subset=-c(20,22))
rstandard(modelB)[20] # 20th in the dataset but 21st data point

hatvalues(modelB)[20]

cooks.distance(modelB)[20]

############################ NOVEMBER 9 ####################################
liquid = read.csv(choose.files(), header = T) # File name is 'Liquidity'
attach(liquid)

# Collinearity Example
m1 = lm(VOLUME ~ AVGT + NTRAN + PRICE + SHARE + VALUE + DEBEQ)
summary(m1)

# Note that the F p-value = 0, which means the model as a whole is highly significant. However, 4 out of the 6
#   t p-values (.54562, .22060, .47591, .30646) are greater than any alpha. So, those 4 X's are all insignificant.
#   This may make us suspicious of collinearity. Let's now investigate!

# Let's start with a correlation matrix between only the explanatory variables.
cor(cbind(AVGT,NTRAN,PRICE,SHARE,VALUE,DEBEQ))
# SHARE does have some pretty high correlations with NTRAN (.817) and VALUE (.829).

# Now let's look at the variance inflation factors. To do that, you will need to install 1 of the following
#   packages: car, faraway, hh, or fmsb.

library(car)
# The library command will 'call' the package you just installed. In the future, you should only need to call 
#   it and not install it again
vif(m1)
# None of these vif value sare > 10, so we do not have severe multicollinearity here. 

# If we had categorical variables in the model, we would get gvif (for generalized vifs) and gvif raised
#   to the power of something B-). That something is $ adjustment for the number of terms within each variable.
#   We will use those if R gives them to us. (they will be in the far right hand column). The cutoff is still 10.

liquid = read.csv(choose.files(), header = T) # File name is 'Liquidity'
attach(liquid)

############################### NOVEMBER 11#############################################
liquid = read.csv(choose.files(), header = T) # File name is 'Liquidity'
attach(liquid)
m1 = lm(VOLUME ~ AVGT + NTRAN + PRICE + SHARE + VALUE + DEBEQ)

plot(residuals(m1) ~ fitted.values(m1), main = "Residuals vs Y-hat Values")
# There is unequal spread here, which is bad and we have heterosedasticity. It has small spread on both ends,
#   medium spread around 10-20, and large spread around 32. 
#   That is what we want errors to look like. This random scatter addresses the linearity assumption of 
#   regression while homoscedasticity addresses the equal variance assumption of regression. 